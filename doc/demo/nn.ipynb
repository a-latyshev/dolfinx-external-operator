{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class convexLinear(torch.nn.Module):\n",
    "    \"\"\" Custom linear layer with positive weights and no bias \"\"\"\n",
    "    def __init__(self, size_in, size_out, ):\n",
    "        super().__init__()\n",
    "        self.size_in, self.size_out = size_in, size_out\n",
    "        weights = torch.Tensor(size_out, size_in)\n",
    "        self.weights = torch.nn.Parameter(weights)\n",
    "\n",
    "        # initialize weights\n",
    "        torch.nn.init.kaiming_uniform_(self.weights, a=np.sqrt(5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        w_times_x= torch.mm(x, torch.nn.functional.softplus(self.weights.t()))\n",
    "        return w_times_x\n",
    "\n",
    "class ICNN(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output, dropout):\n",
    "        super(ICNN, self).__init__()\n",
    "        # Create Module dicts for the hidden and skip-connection layers\n",
    "        self.layers = torch.nn.ModuleDict()\n",
    "        self.skip_layers = torch.nn.ModuleDict()\n",
    "        self.depth = len(n_hidden)\n",
    "        self.dropout = dropout[0]\n",
    "        self.p_dropout = dropout[1]\n",
    "\n",
    "        self.layers[str(0)] = torch.nn.Linear(n_input, n_hidden[0]).float()\n",
    "        # Create create NN with number of elements in n_hidden as depth\n",
    "        for i in range(1, self.depth):\n",
    "            self.layers[str(i)] = convexLinear(n_hidden[i-1], n_hidden[i]).float()\n",
    "            self.skip_layers[str(i)] = torch.nn.Linear(n_input, n_hidden[i]).float()\n",
    "\n",
    "        self.layers[str(self.depth)] = convexLinear(n_hidden[self.depth-1], n_output).float()\n",
    "        self.skip_layers[str(self.depth)] = convexLinear(n_input, n_output).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Get F components\n",
    "        F11 = x[:,0:1]\n",
    "        F12 = x[:,1:2]\n",
    "        F21 = x[:,2:3]\n",
    "        F22 = x[:,3:4]\n",
    "\n",
    "        # Compute right Cauchy green strain Tensor\n",
    "        C11 = F11**2 + F21**2\n",
    "        C12 = F11*F12 + F21*F22\n",
    "        C21 = F11*F12 + F21*F22\n",
    "        C22 = F12**2 + F22**2\n",
    "\n",
    "        # Compute computeStrainInvariants\n",
    "        I1 = C11 + C22 + 1.0\n",
    "        I2 = C11 + C22 - C12*C21 + C11*C22\n",
    "        I3 = C11*C22 - C12*C21\n",
    "\n",
    "        # Apply transformation to invariants\n",
    "        K1 = I1 * torch.pow(I3,-1/3) - 3.0\n",
    "        K2 = (I1 + I3 - 1) * torch.pow(I3,-2/3) - 3.0\n",
    "        J = torch.sqrt(I3)\n",
    "        K3 = (J-1)**2\n",
    "\n",
    "        # Concatenate feature\n",
    "        x_input = torch.cat((K1,K2,K3),1).float()\n",
    "\n",
    "        z = x_input.clone()\n",
    "        z = self.layers[str(0)](z)\n",
    "        for layer in range(1,self.depth):\n",
    "            skip = self.skip_layers[str(layer)](x_input)\n",
    "            z = self.layers[str(layer)](z)\n",
    "            z += skip\n",
    "            z = torch.nn.functional.softplus(z)     \n",
    "            z = 1/12.*torch.square(z)\n",
    "            if self.training:\n",
    "                if self.dropout:\n",
    "                    z = torch.nn.functional.dropout(z,p=self.p_dropout)\n",
    "        y = self.layers[str(self.depth)](z) + self.skip_layers[str(self.depth)](x_input)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y0/23jtbc011ldb7ngvjccl2w9sr08f79/T/ipykernel_44847/363596085.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('ArrudaBoyce_noise=high.pth'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_input = 3\n",
    "n_output = 1\n",
    "n_hidden = [64,64,64]\n",
    "dropout = [True,0.2]\n",
    "model = ICNN(n_input=n_input,\n",
    "                n_hidden=n_hidden,\n",
    "                n_output=n_output,\n",
    "                dropout=dropout)\n",
    "\n",
    "model.load_state_dict(torch.load('ArrudaBoyce_noise=high.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Create dummy deformation gradients based on uniaxial tension\n",
    "F=torch.zeros(50,4)\n",
    "gamma=torch.linspace(0,0.5,50)\n",
    "for a in range(50):\n",
    "    F[a,0] = 1 + gamma[a]\n",
    "    F[a,1] = 0\n",
    "    F[a,2] = 0\n",
    "    F[a,3] = 1\n",
    "\n",
    "# Zero input deformation gradient + track gradients\n",
    "F_0 = torch.zeros((1,4))\n",
    "F_0[:,0] = 1\n",
    "F_0[:,3] = 1\n",
    "\n",
    "F11_0 = F_0[:,0:1]\n",
    "F12_0 = F_0[:,1:2]\n",
    "F21_0 = F_0[:,2:3]\n",
    "F22_0 = F_0[:,3:4]\n",
    "\n",
    "F11_0.requires_grad = True\n",
    "F12_0.requires_grad = True\n",
    "F21_0.requires_grad = True\n",
    "F22_0.requires_grad = True\n",
    "\n",
    "\n",
    "# Get components of F of Uniaxial tension + track gradients\n",
    "F11 = F[:,0:1]\n",
    "F12 = F[:,1:2]\n",
    "F21 = F[:,2:3]\n",
    "F22 = F[:,3:4]\n",
    "\n",
    "F11.requires_grad = True\n",
    "F12.requires_grad = True\n",
    "F21.requires_grad = True\n",
    "F22.requires_grad = True\n",
    "\n",
    "# Predict strain energy (uncorrected)\n",
    "W_NN = model(torch.cat((F11,F12,F21,F22),dim=1))\n",
    "\n",
    "# Get dW_NN/DF components\n",
    "dW_NN_dF11 = torch.autograd.grad(W_NN,F11,torch.ones(F11.shape[0],1),create_graph=True)[0]\n",
    "dW_NN_dF12 = torch.autograd.grad(W_NN,F12,torch.ones(F12.shape[0],1),create_graph=True)[0]\n",
    "dW_NN_dF21 = torch.autograd.grad(W_NN,F21,torch.ones(F21.shape[0],1),create_graph=True)[0]\n",
    "dW_NN_dF22 = torch.autograd.grad(W_NN,F22,torch.ones(F22.shape[0],1),create_graph=True)[0]\n",
    "\n",
    "# Assemble first PK stress tensor as shape (-1,4)\n",
    "P_NN = torch.cat((dW_NN_dF11,dW_NN_dF12,dW_NN_dF21,dW_NN_dF22),dim=1)\n",
    "\n",
    "# Predict energy at zero deformation for energy correction\n",
    "W_NN_0 = model(torch.cat((F11_0,F12_0,F21_0,F22_0),dim=1))\n",
    "\n",
    "# Predict stress at zero deformation for stress correction\n",
    "dW_NN_dF11_0 = torch.autograd.grad(W_NN_0,F11_0,torch.ones(F11_0.shape[0],1),create_graph=True)[0]\n",
    "dW_NN_dF12_0 = torch.autograd.grad(W_NN_0,F12_0,torch.ones(F12_0.shape[0],1),create_graph=True)[0]\n",
    "dW_NN_dF21_0 = torch.autograd.grad(W_NN_0,F21_0,torch.ones(F21_0.shape[0],1),create_graph=True)[0]\n",
    "dW_NN_dF22_0 = torch.autograd.grad(W_NN_0,F22_0,torch.ones(F22_0.shape[0],1),create_graph=True)[0]\n",
    "\n",
    "P_NN_0 = torch.cat((dW_NN_dF11_0,dW_NN_dF12_0,dW_NN_dF21_0,dW_NN_dF22_0),dim=1)\n",
    "\n",
    "# Assemble stress correction term\n",
    "P_cor = torch.zeros_like(P_NN)\n",
    "\n",
    "P_cor[:,0:1] = F11*-P_NN_0[:,0:1] + F12*-P_NN_0[:,2:3]\n",
    "P_cor[:,1:2] = F11*-P_NN_0[:,1:2] + F12*-P_NN_0[:,3:4]\n",
    "P_cor[:,2:3] = F21*-P_NN_0[:,0:1] + F22*-P_NN_0[:,2:3]\n",
    "P_cor[:,3:4] = F21*-P_NN_0[:,1:2] + F22*-P_NN_0[:,3:4]\n",
    "\n",
    "# obtain final stress\n",
    "P = P_NN + P_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_NN_0.reshape(2,2) @ P_NN_0.reshape(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voigt_map = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.int32)\n",
    "# delta = torch.eye(2)\n",
    "\n",
    "# delta_H = torch.zeros(num_components, num_components, requires_grad=True)\n",
    "# H = P_NN_0.reshape(2,2)\n",
    "# for c in range(num_components):\n",
    "#     row = []\n",
    "#     for s in range(num_components):\n",
    "#         row.append(delta[Voigt_map[c][0], Voigt_map[s][0]] * H[Voigt_map[s][1], Voigt_map[c][1]])\n",
    "#     delta_H[c] = torch.tensor(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian shape: torch.Size([50, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all components of F into a single tensor\n",
    "F = torch.cat((F11, F12, F21, F22), dim=1)\n",
    "\n",
    "# Predict strain energy (uncorrected)\n",
    "W_NN = model(F)\n",
    "\n",
    "# Compute the gradient of W_NN with respect to the entire F tensor\n",
    "P_NN = torch.autograd.grad(W_NN, F, torch.ones_like(W_NN), create_graph=True)[0]\n",
    "\n",
    "F_0 = torch.cat((F11_0,F12_0,F21_0,F22_0),dim=1)\n",
    "W_NN_0 = model(F_0)\n",
    "P_NN_0 = torch.autograd.grad(W_NN_0, F_0, torch.ones_like(W_NN_0), create_graph=True)[0].squeeze()\n",
    "\n",
    "P_cor = torch.zeros_like(P_NN)\n",
    "\n",
    "P_cor[:,0] = F[:,0]*-P_NN_0[0] + F[:,1]*-P_NN_0[2]\n",
    "P_cor[:,1] = F[:,0]*-P_NN_0[1] + F[:,1]*-P_NN_0[3]\n",
    "P_cor[:,2] = F[:,2]*-P_NN_0[0] + F[:,3]*-P_NN_0[2]\n",
    "P_cor[:,3] = F[:,2]*-P_NN_0[1] + F[:,3]*-P_NN_0[3]\n",
    "\n",
    "P = P_NN + P_cor\n",
    "\n",
    "# Initialize a tensor to store the full Jacobian (second derivative)\n",
    "batch_size, num_components = F.shape\n",
    "jacobian_rows = []\n",
    "\n",
    "# Compute the Jacobian row by row\n",
    "for i in range(num_components):\n",
    "    grad_output = torch.zeros_like(P)\n",
    "    grad_output[:, i] = 1.0  # Select the i-th component\n",
    "    jacobian_rows.append(torch.autograd.grad(P, F, grad_output, create_graph=True)[0])\n",
    "\n",
    "dP = torch.stack(jacobian_rows, dim=1)  # Shape: (batch_size, num_components, num_components)\n",
    "\n",
    "print(\"Jacobian shape:\", dP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_NN_0.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_NN_0[:,0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian shape: torch.Size([50, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Stack the rows to form the full Jacobian tensor\n",
    "dP_NN = torch.stack(jacobian_rows, dim=1)  # Shape: (batch_size, num_components, num_components)\n",
    "\n",
    "print(\"Jacobian shape:\", dP_NN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x\n",
    "torch.stack([x,x,x]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW_NN_dF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0656, 0.0000, 0.0000, 0.0125],\n",
       "        [0.1302, 0.0000, 0.0000, 0.0258],\n",
       "        [0.1940, 0.0000, 0.0000, 0.0397],\n",
       "        [0.2570, 0.0000, 0.0000, 0.0544],\n",
       "        [0.3192, 0.0000, 0.0000, 0.0698],\n",
       "        [0.3806, 0.0000, 0.0000, 0.0858],\n",
       "        [0.4412, 0.0000, 0.0000, 0.1026],\n",
       "        [0.5012, 0.0000, 0.0000, 0.1201],\n",
       "        [0.5604, 0.0000, 0.0000, 0.1382],\n",
       "        [0.6190, 0.0000, 0.0000, 0.1571],\n",
       "        [0.6769, 0.0000, 0.0000, 0.1766],\n",
       "        [0.7342, 0.0000, 0.0000, 0.1968],\n",
       "        [0.7909, 0.0000, 0.0000, 0.2177],\n",
       "        [0.8471, 0.0000, 0.0000, 0.2393],\n",
       "        [0.9026, 0.0000, 0.0000, 0.2616],\n",
       "        [0.9576, 0.0000, 0.0000, 0.2846],\n",
       "        [1.0121, 0.0000, 0.0000, 0.3082],\n",
       "        [1.0661, 0.0000, 0.0000, 0.3325],\n",
       "        [1.1196, 0.0000, 0.0000, 0.3575],\n",
       "        [1.1725, 0.0000, 0.0000, 0.3832],\n",
       "        [1.2251, 0.0000, 0.0000, 0.4095],\n",
       "        [1.2772, 0.0000, 0.0000, 0.4365],\n",
       "        [1.3288, 0.0000, 0.0000, 0.4642],\n",
       "        [1.3800, 0.0000, 0.0000, 0.4925],\n",
       "        [1.4308, 0.0000, 0.0000, 0.5215],\n",
       "        [1.4812, 0.0000, 0.0000, 0.5512],\n",
       "        [1.5313, 0.0000, 0.0000, 0.5816],\n",
       "        [1.5809, 0.0000, 0.0000, 0.6126],\n",
       "        [1.6302, 0.0000, 0.0000, 0.6442],\n",
       "        [1.6791, 0.0000, 0.0000, 0.6765],\n",
       "        [1.7277, 0.0000, 0.0000, 0.7095],\n",
       "        [1.7759, 0.0000, 0.0000, 0.7432],\n",
       "        [1.8238, 0.0000, 0.0000, 0.7775],\n",
       "        [1.8714, 0.0000, 0.0000, 0.8124],\n",
       "        [1.9187, 0.0000, 0.0000, 0.8480],\n",
       "        [1.9657, 0.0000, 0.0000, 0.8843],\n",
       "        [2.0124, 0.0000, 0.0000, 0.9212],\n",
       "        [2.0588, 0.0000, 0.0000, 0.9588],\n",
       "        [2.1050, 0.0000, 0.0000, 0.9970],\n",
       "        [2.1508, 0.0000, 0.0000, 1.0358],\n",
       "        [2.1964, 0.0000, 0.0000, 1.0753],\n",
       "        [2.2418, 0.0000, 0.0000, 1.1155],\n",
       "        [2.2869, 0.0000, 0.0000, 1.1563],\n",
       "        [2.3317, 0.0000, 0.0000, 1.1978],\n",
       "        [2.3764, 0.0000, 0.0000, 1.2398],\n",
       "        [2.4207, 0.0000, 0.0000, 1.2826],\n",
       "        [2.4649, 0.0000, 0.0000, 1.3259],\n",
       "        [2.5089, 0.0000, 0.0000, 1.3700],\n",
       "        [2.5526, 0.0000, 0.0000, 1.4146]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "P11 = torch.autograd.grad(W_NN, F11, torch.ones(F11.shape[0],1), create_graph=True)[0]\n",
    "dP11 = torch.autograd.grad(P11, F11, torch.ones(F11.shape[0],1), create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([50, 1]) and output[0] has a shape of torch.Size([50, 4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mP_NN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF11\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdW_NN_dF11\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/__init__.py:469\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    460\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    468\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(outputs))\n\u001b[0;32m--> 469\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/__init__.py:163\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    146\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `is_grads_batched=True`, we interpret the first \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension of each grad_output as the batch dimension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched, consider using vmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    164\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch in shape: grad_output[\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(grads\u001b[38;5;241m.\u001b[39mindex(grad))\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m] has a shape of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(grad_shape)\n\u001b[1;32m    168\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and output[\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mindex(out))\n\u001b[1;32m    170\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m] has a shape of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(out_shape)\n\u001b[1;32m    172\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         )\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype\u001b[38;5;241m.\u001b[39mis_complex \u001b[38;5;241m!=\u001b[39m grad\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_complex:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor complex Tensors, both grad_output and output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are required to have the same dtype.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([50, 1]) and output[0] has a shape of torch.Size([50, 4])."
     ]
    }
   ],
   "source": [
    "tmp = torch.autograd.grad(P_NN, F11, torch.ones_like(dW_NN_dF11), create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2W_NN_dF11_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW_NN_dF11_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_NN_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW_NN_dF11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW_NN_dF11 = torch.autograd.grad(W_NN,F11,torch.ones(F11.shape[0],1),create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dP_dF11 = torch.autograd.grad(W_NN,F11,torch.ones(F11.shape[0],1),create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((F11,F12,F21,F22),dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_0[:,3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_0[:,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((F11_0,F12_0,F21_0,F22_0),dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_0.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_NN_0 = model(F_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(F_0.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 4]) and output[0] has a shape of torch.Size([1, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m F_0\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dW_NN \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW_NN_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m dW_NN\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/__init__.py:469\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    460\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    468\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(outputs))\n\u001b[0;32m--> 469\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/__init__.py:163\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    146\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `is_grads_batched=True`, we interpret the first \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension of each grad_output as the batch dimension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched, consider using vmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    164\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch in shape: grad_output[\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(grads\u001b[38;5;241m.\u001b[39mindex(grad))\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m] has a shape of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(grad_shape)\n\u001b[1;32m    168\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and output[\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mindex(out))\n\u001b[1;32m    170\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m] has a shape of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(out_shape)\n\u001b[1;32m    172\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         )\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype\u001b[38;5;241m.\u001b[39mis_complex \u001b[38;5;241m!=\u001b[39m grad\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_complex:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor complex Tensors, both grad_output and output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are required to have the same dtype.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([1, 4]) and output[0] has a shape of torch.Size([1, 1])."
     ]
    }
   ],
   "source": [
    "F_0.requires_grad = True\n",
    "\n",
    "dW_NN = torch.autograd.grad(W_NN_0, F_0, torch.ones_like(F_0), create_graph=True)[0]\n",
    "dW_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
